
defaults:
  - domain_randomization: default_dom_rand_conf
  - override hydra/job_logging: default
  - override hydra/launcher: basic

#hydra:
#  mode: MULTIRUN
#  job_logging: {}
#  hydra_logging: {}
#  sweeper:
#    params:
##      experiment.learnable_std: true, false
##      experiment.init_std: 0.1, 0.2
##      experiment.num_steps: 10, 20
##      experiment.update_epochs: 4
##      experiment.clip_eps: 0.2, 0.05
#      # env_params
#      experiment.env_params.reward_params.air_time_coeff: 0.25
#      experiment.env_params.reward_params.joint_acc_coeff: 2e-7, 3e-7
#      experiment.env_params.reward_params.air_time_max: 0.1, 0.5
#      experiment.env_params.reward_params.joint_position_limit_coeff: 1.0, 5.0
#      experiment.env_params.reward_params.action_rate_coeff: 1e-2, 1e-3

wandb:
  project: "unitreeA1_wyaw"

experiment:
  task_factory:
    name: RLFactory
  env_params:
    env_name: MjxUnitreeA1
    horizon: 1000
    terminal_state_type: HeightBasedTerminalStateHandler
    goal_type: GoalRandomRootVelocity
    reward_type: LocomotionReward
    reward_params:
        air_time_coeff: 0.0
        joint_acc_coeff: 2e-7
        air_time_max: 0.5
        joint_position_limit_coeff: 5.0
        action_rate_coeff: 0.01
    #terrain_type: RoughTerrain
    init_state_type: TrajInitialStateHandler
    #domain_randomization_type: DefaultRandomizer
    #domain_randomization_params: ${domain_randomization}
  hidden_layers: [512, 256]
  lr: 1e-4
  num_envs: 2048
  num_steps: 20
  total_timesteps: 5e7
  update_epochs: 4
  num_minibatches: 32
  gamma: 0.99
  gae_lambda: 0.95
  clip_eps: 0.2
  init_std: 0.2
  learnable_std: false
  ent_coef: 0.0
  vf_coef: 0.5
  max_grad_norm: 0.5
  activation: tanh
  anneal_lr: false
  weight_decay: 0.0
  normalize_env: true
  debug: false
  n_seeds: 1  # while automatically take seeds from 1 to n_seeds
  vmap_across_seeds: true
  validation:
    active: false
    num_steps: 100
    num_envs: 100
    num: 10 # set to 0 to disable validation
